{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d9db16",
   "metadata": {},
   "source": [
    "# Fast-dLLM LLaDA Model Demo\n",
    "\n",
    "本Notebook演示如何使用Fast-dLLM的LLaDA模型进行文本生成，包括：\n",
    "- 基础生成\n",
    "- 使用Prefix Cache加速\n",
    "- 使用Dual Cache加速\n",
    "- 使用并行解码加速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe50121",
   "metadata": {},
   "source": [
    "## 1. 环境准备\n",
    "\n",
    "首先导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcef3671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.9.1+cu128\n",
      "CUDA是否可用: True\n",
      "GPU设备: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加llada目录到Python路径\n",
    "sys.path.append('/home/jovyan/Fast-dLLM/llada')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from model.modeling_llada import LLaDAModelLM\n",
    "from generate import generate, generate_with_prefix_cache, generate_with_dual_cache\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1964aed",
   "metadata": {},
   "source": [
    "## 2. 加载模型和Tokenizer\n",
    "\n",
    "加载LLaDA-8B-Instruct模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb048f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "正在加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载模型\n",
    "print(\"正在加载模型...\")\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    'GSAI-ML/LLaDA-8B-Instruct', \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device).eval()\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'GSAI-ML/LLaDA-8B-Instruct', \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"模型加载完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b2e890",
   "metadata": {},
   "source": [
    "## 3. 准备输入\n",
    "\n",
    "准备一个测试问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc90b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: What is the capital of France?\n",
      "\n",
      "格式化后的提示:\n",
      "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "输入token数: 20\n"
     ]
    }
   ],
   "source": [
    "# 准备问题\n",
    "question = \"What is the capital of France?\"\n",
    "print(f\"问题: {question}\")\n",
    "\n",
    "# 应用chat模板\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(f\"\\n格式化后的提示:\\n{prompt_text}\")\n",
    "\n",
    "# Tokenize\n",
    "input_ids = tokenizer(prompt_text)['input_ids']\n",
    "prompt = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "print(f\"\\n输入token数: {prompt.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594241b",
   "metadata": {},
   "source": [
    "## 4. 方法1: 基础生成（无Cache）\n",
    "\n",
    "使用标准的LLaDA生成方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5985c81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法1: 基础生成（无Cache）\n",
      "生成长度: 128, 采样步数: 128, 块大小: 32\n",
      "生成中...\n",
      "\n",
      "生成结果: The capital of France is Paris.\n",
      "前向传播次数: 128\n",
      "生成时间: 9.78秒\n",
      "吞吐量: 13.08 tokens/s\n"
     ]
    }
   ],
   "source": [
    "# 生成参数\n",
    "gen_length = 128  # 生成长度\n",
    "steps = 128       # 采样步数\n",
    "block_size = 32   # 块大小\n",
    "\n",
    "print(\"方法1: 基础生成（无Cache）\")\n",
    "print(f\"生成长度: {gen_length}, 采样步数: {steps}, 块大小: {block_size}\")\n",
    "print(\"生成中...\")\n",
    "\n",
    "start_time = time.time()\n",
    "out, nfe = generate(\n",
    "    model, \n",
    "    prompt, \n",
    "    steps=steps, \n",
    "    gen_length=gen_length, \n",
    "    block_length=block_size, \n",
    "    temperature=0., \n",
    "    remasking='low_confidence'\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# 解码结果\n",
    "answer = tokenizer.batch_decode(out[:, prompt.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"\\n生成结果: {answer}\")\n",
    "print(f\"前向传播次数: {nfe}\")\n",
    "print(f\"生成时间: {elapsed_time:.2f}秒\")\n",
    "print(f\"吞吐量: {gen_length / elapsed_time:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a68ab",
   "metadata": {},
   "source": [
    "## 5. 方法2: 使用Prefix Cache加速\n",
    "\n",
    "通过缓存前缀的KV来加速生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76140be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法2: 使用Prefix Cache\n",
      "生成长度: 128, 采样步数: 128, 块大小: 32\n",
      "生成中...\n",
      "\n",
      "生成结果: The capital of France is Paris.\n",
      "前向传播次数: 128\n",
      "生成时间: 6.87秒\n",
      "吞吐量: 18.62 tokens/s\n"
     ]
    }
   ],
   "source": [
    "print(\"方法2: 使用Prefix Cache\")\n",
    "print(f\"生成长度: {gen_length}, 采样步数: {steps}, 块大小: {block_size}\")\n",
    "print(\"生成中...\")\n",
    "\n",
    "start_time = time.time()\n",
    "out, nfe = generate_with_prefix_cache(\n",
    "    model, \n",
    "    prompt, \n",
    "    steps=steps, \n",
    "    gen_length=gen_length, \n",
    "    block_length=block_size, \n",
    "    temperature=0., \n",
    "    remasking='low_confidence'\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# 解码结果\n",
    "answer = tokenizer.batch_decode(out[:, prompt.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"\\n生成结果: {answer}\")\n",
    "print(f\"前向传播次数: {nfe}\")\n",
    "print(f\"生成时间: {elapsed_time:.2f}秒\")\n",
    "print(f\"吞吐量: {gen_length / elapsed_time:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81d829",
   "metadata": {},
   "source": [
    "## 6. 方法3: 使用Dual Cache加速\n",
    "\n",
    "同时缓存前缀和后缀的KV来进一步加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a5dc33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法3: 使用Dual Cache\n",
      "生成长度: 128, 采样步数: 128, 块大小: 32\n",
      "生成中...\n",
      "\n",
      "生成结果: The capital of France is Paris.\n",
      "前向传播次数: 128\n",
      "生成时间: 8.67秒\n",
      "吞吐量: 14.77 tokens/s\n"
     ]
    }
   ],
   "source": [
    "print(\"方法3: 使用Dual Cache\")\n",
    "print(f\"生成长度: {gen_length}, 采样步数: {steps}, 块大小: {block_size}\")\n",
    "print(\"生成中...\")\n",
    "\n",
    "start_time = time.time()\n",
    "out, nfe = generate_with_dual_cache(\n",
    "    model, \n",
    "    prompt, \n",
    "    steps=steps, \n",
    "    gen_length=gen_length, \n",
    "    block_length=block_size, \n",
    "    temperature=0., \n",
    "    remasking='low_confidence'\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# 解码结果\n",
    "answer = tokenizer.batch_decode(out[:, prompt.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"\\n生成结果: {answer}\")\n",
    "print(f\"前向传播次数: {nfe}\")\n",
    "print(f\"生成时间: {elapsed_time:.2f}秒\")\n",
    "print(f\"吞吐量: {gen_length / elapsed_time:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bb41d",
   "metadata": {},
   "source": [
    "## 7. 方法4: Dual Cache + 置信度并行解码\n",
    "\n",
    "结合Dual Cache和基于置信度的并行解码，实现最大加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b55f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法4: Dual Cache + 置信度并行解码\n",
      "生成长度: 128, 采样步数: 128, 块大小: 32\n",
      "置信度阈值: 0.8\n",
      "生成中...\n",
      "\n",
      "生成结果: The capital of France is Paris.\n",
      "前向传播次数: 6\n",
      "生成时间: 0.37秒\n",
      "吞吐量: 346.54 tokens/s\n"
     ]
    }
   ],
   "source": [
    "# 使用置信度阈值进行并行解码\n",
    "threshold = 0.8  # 置信度阈值\n",
    "\n",
    "print(\"方法4: Dual Cache + 置信度并行解码\")\n",
    "print(f\"生成长度: {gen_length}, 采样步数: {steps}, 块大小: {block_size}\")\n",
    "print(f\"置信度阈值: {threshold}\")\n",
    "print(\"生成中...\")\n",
    "\n",
    "start_time = time.time()\n",
    "out, nfe = generate_with_dual_cache(\n",
    "    model, \n",
    "    prompt, \n",
    "    steps=steps, \n",
    "    gen_length=gen_length, \n",
    "    block_length=block_size, \n",
    "    temperature=0., \n",
    "    remasking='low_confidence',\n",
    "    threshold=threshold  # 启用并行解码\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# 解码结果\n",
    "answer = tokenizer.batch_decode(out[:, prompt.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"\\n生成结果: {answer}\")\n",
    "print(f\"前向传播次数: {nfe}\")\n",
    "print(f\"生成时间: {elapsed_time:.2f}秒\")\n",
    "print(f\"吞吐量: {gen_length / elapsed_time:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e99a03",
   "metadata": {},
   "source": [
    "## 8. 尝试其他问题\n",
    "\n",
    "您可以修改下面的问题来测试不同的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55032c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: Explain quantum computing in simple terms.\n",
      "\n",
      "回答: Quantum computing is a type of computing that uses the principles of quantum mechanics to perform calculations. In classical computers, the smallest unit of is a bit, which can be either a 0 or a 1. In quantum computing, the smallest unit is called a quantum bit, or qubit. Qubits can be both 0 and 1 at the same time, thanks to a property called superposition. This allows quantum computers to perform many calculations at once, making them much faster than classical computers.\n",
      "\n",
      "Another important concept in quantum computing is entanglement, where two more qubits in such a way that the state of one qubit can affect the state of another, even if they are far apart. This allows quantum computers to perform computers.\n",
      "\n",
      "In summary, quantum computing uses the principles of quantum mechanics to perform calculations much faster and classical\n",
      "\n",
      "生成时间: 7.72秒\n",
      "前向传播次数: 105\n"
     ]
    }
   ],
   "source": [
    "# 自定义您的问题\n",
    "custom_question = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "# 准备输入\n",
    "messages = [{\"role\": \"user\", \"content\": custom_question}]\n",
    "prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_ids = tokenizer(prompt_text)['input_ids']\n",
    "prompt = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "\n",
    "print(f\"问题: {custom_question}\\n\")\n",
    "\n",
    "# 使用最快的方法生成\n",
    "start_time = time.time()\n",
    "out, nfe = generate_with_dual_cache(\n",
    "    model, \n",
    "    prompt, \n",
    "    steps=128, \n",
    "    gen_length=256,  # 更长的生成\n",
    "    block_length=32, \n",
    "    temperature=0., \n",
    "    remasking='low_confidence',\n",
    "    threshold=0.8\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "answer = tokenizer.batch_decode(out[:, prompt.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"回答: {answer}\")\n",
    "print(f\"\\n生成时间: {elapsed_time:.2f}秒\")\n",
    "print(f\"前向传播次数: {nfe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6321a8",
   "metadata": {},
   "source": [
    "## 9. 多轮对话示例\n",
    "\n",
    "演示如何进行多轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26487a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户: What is machine learning?\n",
      "助手: Machine learning is a subset of artificial intelligence that involves the development of algorithms that enable computers to learn from data and make predictions or decisions based on that data. It is a method of training computers to perform tasks without being explicitly programmed. Instead, machine learning algorithms are designed to learn from data and improve their performance over time. This is achieved through the use of statistical techniques and mathematical models that allow computers to identify patterns in data and make predictions or decisions based on those patterns. Machine learning has numerous applications in various fields, including healthcare, finance, marketing, and more, and is an important component of modern data science and artificial intelligence.\n",
      "\n",
      "用户: Can you give me an example?\n",
      "助手: Sure! One example of machine learning is the use of image recognition algorithms. These algorithms can be trained to recognize patterns in images, such as objects or faces, and make predictions or decisions based on those patterns. For instance, a machine learning algorithm can be trained to recognize images of cats and dogs by analyzing a large dataset of images and identifying patterns that distinguish between the two. Once the algorithm has been trained, it can be used to classify new images as either cats or dogs. This type of machine learning application is widely used in fields such as healthcare, security, and entertainment, and has the potential to revolutionize industries and society.\n"
     ]
    }
   ],
   "source": [
    "# 第一轮对话\n",
    "question1 = \"What is machine learning?\"\n",
    "messages = [{\"role\": \"user\", \"content\": question1}]\n",
    "prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_ids = tokenizer(prompt_text)['input_ids']\n",
    "conversation = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "\n",
    "print(f\"用户: {question1}\")\n",
    "\n",
    "# 生成回答\n",
    "out, nfe = generate_with_dual_cache(\n",
    "    model, conversation, steps=128, gen_length=128, \n",
    "    block_length=32, temperature=0., remasking='low_confidence', threshold=0.8\n",
    ")\n",
    "answer1 = tokenizer.batch_decode(out[:, conversation.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"助手: {answer1}\\n\")\n",
    "\n",
    "# 移除EOS token并更新对话历史\n",
    "conversation = out[out != 126081].unsqueeze(0)\n",
    "\n",
    "# 第二轮对话\n",
    "question2 = \"Can you give me an example?\"\n",
    "messages = [{\"role\": \"user\", \"content\": question2}]\n",
    "prompt_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_ids = tokenizer(prompt_text)['input_ids']\n",
    "new_input = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "conversation = torch.cat([conversation, new_input[:, 1:]], dim=1)\n",
    "\n",
    "print(f\"用户: {question2}\")\n",
    "\n",
    "# 生成回答\n",
    "out, nfe = generate_with_dual_cache(\n",
    "    model, conversation, steps=128, gen_length=128, \n",
    "    block_length=32, temperature=0., remasking='low_confidence', threshold=0.8\n",
    ")\n",
    "answer2 = tokenizer.batch_decode(out[:, conversation.shape[1]:], skip_special_tokens=True)[0]\n",
    "print(f\"助手: {answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274ed32",
   "metadata": {},
   "source": [
    "## 10. 参数说明\n",
    "\n",
    "主要参数说明：\n",
    "\n",
    "- **gen_length**: 生成的最大token数\n",
    "- **steps**: 扩散采样步数（通常与gen_length相同或接近）\n",
    "- **block_size**: 块解码的大小，影响Cache效率\n",
    "- **threshold**: 置信度阈值（0-1），用于并行解码。设置为None则禁用并行解码\n",
    "- **temperature**: 温度参数，控制随机性（0表示确定性生成）\n",
    "- **remasking**: 重mask策略，'low_confidence'表示对低置信度token重新mask\n",
    "\n",
    "### 性能优化建议：\n",
    "\n",
    "1. **基础生成**: 适用于需要最高质量的场景\n",
    "2. **Prefix Cache**: 2-3x加速，适用于大多数场景\n",
    "3. **Dual Cache**: 进一步加速，轻微质量损失\n",
    "4. **Dual Cache + 并行解码**: 最大加速(8-11x)，适用于速度优先场景"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
